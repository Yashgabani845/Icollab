# -*- coding: utf-8 -*-
"""chat_summarizer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1M4yV8MY1-kDvAyVErn0C5nSQUBSLivvm

# **Step 1: Set Up Google Colab Environment**

To create a model for summarizing chats through fine-tuning, you'll need a step-by-step approach. We'll use a pre-trained model (like OpenAI's GPT or Hugging Face's transformers) and fine-tune it on your specific dataset to summarize large chats effectively. Here's a complete guide:

Step 1: Set Up Google Colab Environment
Open Google Colab and create a new notebook.
Ensure you have a GPU runtime enabled (Runtime > Change runtime type > Hardware Accelerator > GPU).

# **Step 2: Install Required Libraries**
"""

# !pip install transformers datasets accelerate
# !pip install wandb

"""# **Step 3: Prepare Your Dataset**"""

import csv
import random

# Generate a large dataset with dialogue and summary pairs
def generate_large_dataset(num_samples=1000):
    data = []
    for i in range(num_samples):
        dialogue = f"User: Hello, this is message {i}. How are you?\nAssistant: I'm fine, thank you. How can I assist you with message {i}?"
        summary = f"User greeted and inquired about assistance for message {i}."
        data.append({"dialogue": dialogue, "summary": summary})
    return data

# Create the dataset
data = generate_large_dataset(1000)

# Write the dataset to a CSV file
csv_file_path = "large_dialogue_summary_dataset.csv"

with open(csv_file_path, mode="w", newline="", encoding="utf-8") as file:
    writer = csv.DictWriter(file, fieldnames=["dialogue", "summary"])
    writer.writeheader()
    writer.writerows(data)

print(f"CSV file has been created: {csv_file_path}")

from datasets import load_dataset

# Replace 'your_dataset_path.csv' with your uploaded dataset path
dataset = load_dataset('csv', data_files='large_dialogue_summary_dataset.csv')
print(dataset)

# Split into train and validation datasets
train_dataset = dataset['train'].train_test_split(test_size=0.1)['train']
val_dataset = dataset['train'].train_test_split(test_size=0.1)['test']

"""# **Step 4: Choose a Pre-trained Model**"""

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

model_name = "facebook/bart-large-cnn"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

"""# **Step 5: Preprocess the Data**"""

def preprocess_function(examples):
    inputs = tokenizer(examples['dialogue'], max_length=1024, truncation=True, padding='max_length')
    outputs = tokenizer(examples['summary'], max_length=128, truncation=True, padding='max_length')
    inputs['labels'] = outputs['input_ids']
    return inputs

tokenized_train = train_dataset.map(preprocess_function, batched=True)
tokenized_val = val_dataset.map(preprocess_function, batched=True)

"""# **Step 6: Define the Data Collator**"""

from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)

"""# **Step 7: Set Up Training Arguments**"""

import os
from transformers import Seq2SeqTrainingArguments

# Disable Weights & Biases (wandb)
os.environ["WANDB_DISABLED"] = "true"

# Set up training arguments
training_args = Seq2SeqTrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    num_train_epochs=3,
    weight_decay=0.01,
    save_total_limit=2,
    predict_with_generate=True,
    fp16=True,  # Use mixed precision for faster training on GPU
    logging_dir='./logs',
    logging_steps=10,
)

"""# **Step 8: Train the Model**"""

from transformers import Seq2SeqTrainer, DataCollatorForSeq2Seq

# Define Data Collator for padding and batch consistency
data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)

# Initialize the Trainer
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,  # Replace with your tokenized training dataset
    eval_dataset=tokenized_val,     # Replace with your tokenized validation dataset
    tokenizer=tokenizer,
    data_collator=data_collator,
)

# Start training
trainer.train()

# Save the final model
trainer.save_model("./final_chat_summary_model")
tokenizer.save_pretrained("./final_chat_summary_model")

print("Training completed! The fine-tuned model is saved to './final_chat_summary_model'.")

"""# **Step 9: Evaluate the Model**"""

# !pip install evaluate

# Install the required dependency
# !pip install rouge_score

# Now import and use the ROUGE metric
import evaluate
from datasets import load_dataset

# Load ROUGE metric
rouge = evaluate.load("rouge")

# Function to compute metrics during evaluation
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = tokenizer.batch_decode(logits, skip_special_tokens=True)
    references = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # Compute ROUGE score
    results = rouge.compute(predictions=predictions, references=references)
    return results

"""# **Step 10: Test the Model**"""

# Import torch
import torch

# Make sure to move the model and input tensors to the same device (GPU if available, else CPU)
device = "cuda" if torch.cuda.is_available() else "cpu"

# Move model to the correct device
model = model.to(device)

# Tokenize the input chat and move inputs to the correct device
inputs = tokenizer(chat, return_tensors="pt", max_length=1024, truncation=True).to(device)

# Generate the summary
summary_ids = model.generate(inputs['input_ids'], max_length=128, num_beams=4, early_stopping=True)

# Decode and print the summary
print("Summary:", tokenizer.decode(summary_ids[0], skip_special_tokens=True))